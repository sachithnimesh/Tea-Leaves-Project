{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a2afc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D:\\\\Browns\\\\Tea Leaves Project\\\\Tea leaf dataset\\\\Healthy': 'Exists',\n",
       " 'D:\\\\Browns\\\\Tea Leaves Project\\\\Tea leaf dataset\\\\Tea leaf blight': 'Exists',\n",
       " 'D:\\\\Browns\\\\Tea Leaves Project\\\\Tea leaf dataset\\\\Tea red leaf spot': 'Exists',\n",
       " 'D:\\\\Browns\\\\Tea Leaves Project\\\\Tea leaf dataset\\\\Tea red scab': 'Exists'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the expected dataset directory structure\n",
    "dataset_structure = {\n",
    "    \"Tea leaf dataset\": [\n",
    "        \"Healthy\",\n",
    "        \"Tea leaf blight\",\n",
    "        \"Tea red leaf spot\",\n",
    "        \"Tea red scab\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simulate verification of folder structure (no actual dataset provided)\n",
    "dataset_path = Path(\"D:\\Browns\\Tea Leaves Project\\Tea leaf dataset\")\n",
    "structure_check = {str(dataset_path / category): \"Exists\" for category in dataset_structure[\"Tea leaf dataset\"]}\n",
    "structure_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ff5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3169 images belonging to 4 classes.\n",
      "Found 791 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "DATA_DIR = \"D:\\Browns\\Tea Leaves Project\\Tea leaf dataset\"\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Train and validation generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab5c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SachithN\\AppData\\Local\\Temp\\ipykernel_2012\\1614093806.py:17: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  mobilenet_base = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_model(base_model, name):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name=name)\n",
    "    return model\n",
    "\n",
    "input_tensor = Input(shape=(*IMG_SIZE, 3))\n",
    "\n",
    "# Models\n",
    "efficientnet_base = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "mobilenet_base = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "inception_base = InceptionV3(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "models = {\n",
    "    \"EfficientNetB0\": build_model(efficientnet_base, \"EfficientNetB0\"),\n",
    "    \"MobileNetV2\": build_model(mobilenet_base, \"MobileNetV2\"),\n",
    "    \"InceptionV3\": build_model(inception_base, \"InceptionV3\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e32f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training EfficientNetB0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Browns\\Tea Leaves Project\\.conda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Browns\\Tea Leaves Project\\.conda\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 6s/step - accuracy: 0.9219 - loss: 0.2522 - val_accuracy: 0.5740 - val_loss: 1.7941\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 4s/step - accuracy: 0.9734 - loss: 0.0666 - val_accuracy: 0.1568 - val_loss: 2.7393\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m599s\u001b[0m 6s/step - accuracy: 0.9886 - loss: 0.0446 - val_accuracy: 0.1732 - val_loss: 1.3768\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 5s/step - accuracy: 0.9932 - loss: 0.0231 - val_accuracy: 0.5740 - val_loss: 3.6997\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 5s/step - accuracy: 0.9755 - loss: 0.0668 - val_accuracy: 0.5740 - val_loss: 9.0544\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 5s/step - accuracy: 0.9858 - loss: 0.0375 - val_accuracy: 0.5487 - val_loss: 1.3105\n",
      "Epoch 7/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 6s/step - accuracy: 0.9952 - loss: 0.0134 - val_accuracy: 0.5740 - val_loss: 2.6245\n",
      "Epoch 8/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 5s/step - accuracy: 0.9920 - loss: 0.0234 - val_accuracy: 0.5752 - val_loss: 2.7874\n",
      "Epoch 9/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 5s/step - accuracy: 0.9939 - loss: 0.0226 - val_accuracy: 0.5740 - val_loss: 2.1728\n",
      "Epoch 10/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 5s/step - accuracy: 0.9923 - loss: 0.0296 - val_accuracy: 0.5310 - val_loss: 849.9571\n",
      "Epoch 11/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 6s/step - accuracy: 0.9952 - loss: 0.0169 - val_accuracy: 0.5740 - val_loss: 1.4495\n",
      "\n",
      "Training MobileNetV2...\n",
      "Epoch 1/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 4s/step - accuracy: 0.9164 - loss: 0.2450 - val_accuracy: 0.7092 - val_loss: 2.7596\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 4s/step - accuracy: 0.9520 - loss: 0.1667 - val_accuracy: 0.1492 - val_loss: 18.6089\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 4s/step - accuracy: 0.9829 - loss: 0.0674 - val_accuracy: 0.7054 - val_loss: 4.6521\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 4s/step - accuracy: 0.9914 - loss: 0.0330 - val_accuracy: 0.6675 - val_loss: 4.8030\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 4s/step - accuracy: 0.9882 - loss: 0.0484 - val_accuracy: 0.2971 - val_loss: 10.1406\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 4s/step - accuracy: 0.9701 - loss: 0.1037 - val_accuracy: 0.3552 - val_loss: 8.2907\n",
      "\n",
      "Training InceptionV3...\n",
      "Epoch 1/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 7s/step - accuracy: 0.8768 - loss: 0.3780 - val_accuracy: 0.3401 - val_loss: 6.4074\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 4s/step - accuracy: 0.9595 - loss: 0.1306 - val_accuracy: 0.3489 - val_loss: 5.2463\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 3s/step - accuracy: 0.9733 - loss: 0.0997 - val_accuracy: 0.5158 - val_loss: 18.0627\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 4s/step - accuracy: 0.9661 - loss: 0.1190 - val_accuracy: 0.9532 - val_loss: 1.1257\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 3s/step - accuracy: 0.9820 - loss: 0.0649 - val_accuracy: 0.8255 - val_loss: 0.6587\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 3s/step - accuracy: 0.9798 - loss: 0.0668 - val_accuracy: 0.5879 - val_loss: 9.0948\n",
      "Epoch 7/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 4s/step - accuracy: 0.9773 - loss: 0.0777 - val_accuracy: 0.9785 - val_loss: 0.0641\n",
      "Epoch 8/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 3s/step - accuracy: 0.9864 - loss: 0.0501 - val_accuracy: 0.9520 - val_loss: 0.2048\n",
      "Epoch 9/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 3s/step - accuracy: 0.9894 - loss: 0.0425 - val_accuracy: 0.9823 - val_loss: 0.0594\n",
      "Epoch 10/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 5s/step - accuracy: 0.9905 - loss: 0.0341 - val_accuracy: 0.8850 - val_loss: 1.3934\n",
      "Epoch 11/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 4s/step - accuracy: 0.9864 - loss: 0.0577 - val_accuracy: 0.8432 - val_loss: 0.6163\n",
      "Epoch 12/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 5s/step - accuracy: 0.9809 - loss: 0.0667 - val_accuracy: 0.9975 - val_loss: 0.1250\n",
      "Epoch 13/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 3s/step - accuracy: 0.9953 - loss: 0.0184 - val_accuracy: 0.9949 - val_loss: 0.0190\n",
      "Epoch 14/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4s/step - accuracy: 0.9886 - loss: 0.0435 - val_accuracy: 0.9722 - val_loss: 0.0802\n",
      "Epoch 15/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 6s/step - accuracy: 0.9883 - loss: 0.0456 - val_accuracy: 0.9924 - val_loss: 0.0218\n",
      "Epoch 16/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 6s/step - accuracy: 0.9953 - loss: 0.0173 - val_accuracy: 0.8761 - val_loss: 0.4851\n",
      "Epoch 17/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 7s/step - accuracy: 0.9865 - loss: 0.0603 - val_accuracy: 0.9899 - val_loss: 0.0287\n",
      "Epoch 18/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 6s/step - accuracy: 0.9961 - loss: 0.0139 - val_accuracy: 0.9987 - val_loss: 0.0165\n",
      "Epoch 19/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 6s/step - accuracy: 0.9942 - loss: 0.0235 - val_accuracy: 0.9355 - val_loss: 0.6446\n",
      "Epoch 20/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 6s/step - accuracy: 0.9788 - loss: 0.0629 - val_accuracy: 0.7914 - val_loss: 31.2065\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, name):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    history = model.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[early_stop])\n",
    "    return history, model\n",
    "\n",
    "histories = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    history, trained_model = train_model(model, name)\n",
    "    histories[name] = (history, trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b996b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB0 Final Validation Accuracy: 0.5740\n",
      "MobileNetV2 Final Validation Accuracy: 0.3552\n",
      "InceptionV3 Final Validation Accuracy: 0.7914\n"
     ]
    }
   ],
   "source": [
    "for name, (history, _) in histories.items():\n",
    "    acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"{name} Final Validation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c0679",
   "metadata": {},
   "source": [
    "inseptionv3 is good for this dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
